% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/learnDTR.cont.multi.R
\name{learnDTR.cont.multi}
\alias{learnDTR.cont.multi}
\title{Learning DTR from Sequential Interventions (Continuous Treatment and Multiple outcomes)}
\usage{
learnDTR.cont.multi(
  X,
  A,
  Y,
  weights = NULL,
  weights.Y = NULL,
  baseLearner = c("RF", "BART", "XGBoost", "GAM"),
  metaLearners = c("S"),
  include.X = 0,
  include.A = 0,
  include.Y = 0,
  parallel = FALSE,
  A.box.cnstr = NULL,
  A.cnstr.func = NULL,
  x.select = NULL,
  n.grid = 100,
  verbose = TRUE,
  ...
)
}
\arguments{
\item{X}{A list of information available at each stage in order, that is, \code{X[[1]]} represents the baseline information,
and \code{X[[t]]} is the information observed before \eqn{t^{th}} intervention.
The dimensionality of each element \code{X[[i]]} can be different from each other.
Notably, it can includes previous stages' action information \code{A} and outcome/reward information
\code{Y}. User can flexibly manipulate which covariates to use in training.
However, if argument \code{all.inclusive} is \code{TRUE}, all previous stages' \code{X}, \code{A},
and \code{Y} will be used in training. So, in that case, \code{X} should not involve action and reward
information.}

\item{A}{A list of actions taken during the sequential studies. The order should match with that of \code{X}.
Each element is suppose to be a Nxk matrix, where N is the total number of subjects and
k<=p is the number of actions.}

\item{Y}{A list of outcomes observed in the sequential studies. The order should match with that of \code{X}.
\code{Y[[t]]} is suppose to be driven by the \code{X[[t]]} and action \code{A[[t]]}.
Each element is suppose to be a Nxp matrix, where N is the total number of subjects and
p is the number of outcomes.}

\item{weights}{Weights on each stage of rewards. Default is all 1.}

\item{weights.Y}{The weights for various outcomes. The length should be p. Default is \code{NULL} indicating equal
weights for each outcome.}

\item{baseLearner}{Choose one baselearner for meta-learn er algorithms. So far, it supports \code{BART} by
package \code{dbarts}, \code{RF} (random forests) by \code{ranger}, \code{XGBoost} by
package \code{xgboost}, and \code{GAM} (generalized additive model)
through package \code{glmnet}, which can provide variable selection/sparsity by
various type of regularization. So more in details.}

\item{metaLearners}{\code{c("S")}. For continuous treatment assignment, only S-learner type is available.}

\item{include.X}{0 for no past X included in analysis; 1 for all past X included}

\item{include.A}{0 for no past treatment assignment included in analysis; 1 for only last A included; 2 for all past
A included}

\item{include.Y}{0 for no past reward/outcome Y included in analysis; 1 for only last Y included; 2 for all past
Y included}

\item{parallel}{A boolean, for whether parallel computing is adopted to speed up the "prediction" steps.
It might induce certain connection issue with \code{glmnet}. But parallel is not really
necessary. It does not impact training speed. Also, if a numeric value, it implies the
number of cores to use. Otherwise, directly use the number from \code{detectCores()}.}

\item{A.box.cnstr}{Box constraint for A. Default is \code{NULL}, which means no constraint on A
and the feasible set of A will be implied from observed action values. If there are
k treatment/action, element should be a 2xk matrix}

\item{A.cnstr.func}{For other type of constraint applied to A, user can input a function here.
The basic syntax is \code{function(A, X){...}}, with two arguments: A stands
for action and X are covariates at that stage. For which covariates to select,
use the argument \code{x.select}. The function should return a boolean,
i.e., \code{TRUE} or \code{FALSE}. For example, if constraint is \eqn{3A-log(A)<5},
then function is \code{function(A, X){3A-log(A)<5}}.}

\item{x.select}{Covariate names or id (numeric). Default is \code{NULL} means either all variables are
selected (if X is used in function \code{A.cnstr.func}) or X is not related to A's feasible set}

\item{n.grid}{Number of grid used to search for the best treatment/action. Large values slow down the algorithm.}

\item{verbose}{Console print allowed?}

\item{...}{Additional arguments that can be passed to \code{dbarts::bart}, \code{ranger::ranger},
\code{params} of \code{xbgoost::xgb.cv}, or \code{glmnet::cv.glmnet}}
}
\value{
It includes learning results, basically, the trained functions that can be used for predictions. Since
sequential recommendations might require intermediate observations, \link{learnDTR.cont.multi} will not automatically
provide prediction. But by another function \link{recommendDTR.cont.multi},
predictions can flexibly been made stage by stage.
}
\description{
This function supports to learn the optimal sequential decision rules from either randomized studies
or observational ones. This function support continuous treatment/action values and multiple
outcomes.
}
\details{
This function supports to find the optimal dynamic treatment regime (DTR) for either randomized experiments
or observational studies. Similar to \link{learnDTR.cont}, it supports continuous treatment actions. Moreover,
it allows for multiple outcomes. The objective to maximize turns to be \eqn{w_1y_1 + w_2y_2} given two
outcomes situation and \eqn{w_1} and \eqn{w_2} are coming from input \code{weights.Y}.
Notably, since multiple outcomes, there are also multiple treatment/actions at each stage
corresponding to each outcome.\cr
\cr
It is strongly suggested to adopt BART over random forests as baselearner if sample size is small.
}
\examples{
## Modify dataset to 2 actions and 2 outcomes:
tmp = ThreeStg_Dat
tmp$X = lapply(tmp$X, function(x) as.data.frame(apply(x, 2, scale)))
tmp$X.test = lapply(tmp$X.test, function(x) as.data.frame(apply(x, 2, scale)))
tmp$A = lapply(tmp$A, function(x) cbind(rnorm(400,0,1), rnorm(400,0,2)))
tmp$Y = lapply(tmp$Y, function(x) cbind(x, rnorm(400,0,0.2)+x*runif(400,0,1)))

## Apply the main function to learn the DTRs
DTRs = learnDTR.cont.multi(X = tmp$X,
                          A = tmp$A,
                          Y = tmp$Y,
                          weights = rep(1, 3),
                          weights.Y = c(0.3,0.7),
                          baseLearner  = c("XGBoost"),
                          metaLearners = c("S"),
                          include.X = 1,
                          include.A = 2,
                          include.Y = 0,
                          A.box.cnstr = cbind(c(-2,2), c(-2,2)),
                          A.cnstr.func = function(a, x) {
                            abs(a[,1]+x[,1]) + abs(a[,2]+x[,2]) <= 3
                          },
                          x.select = c("V1", "V2"),
                          n.grid = 50,
                          parallel = FALSE)

}
\references{
Künzel, Sören R., Jasjeet S. Sekhon, Peter J. Bickel, and Bin Yu.
"Metalearners for estimating heterogeneous treatment effects using machine learning."
\emph{Proceedings of the national academy of sciences} 116, no. 10 (2019): 4156-4165.
\cr\cr
Zhou, Junyi, Ying Zhang, and Wanzhu Tu. "A reference-free R-learner for treatment recommendation."
\emph{Statistical Methods in Medical Research} (2022)
}
\seealso{
\code{\link{recommendDTR.cont.multi}}
}
\author{
Junyi Zhou \email{junyzhou@iu.edu}
}
