% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/learnDTR.cont.R
\name{learnDTR.cont}
\alias{learnDTR.cont}
\title{Learning DTR from Sequential Interventions (Continuous Treatment)}
\usage{
learnDTR.cont(
  X,
  A,
  Y,
  weights = rep(1, length(X)),
  baseLearner = c("RF", "BART", "XGBoost", "GAM"),
  metaLearners = c("S"),
  include.X = 0,
  include.A = 0,
  include.Y = 0,
  est.sigma = NULL,
  parallel = FALSE,
  verbose = TRUE,
  ...
)
}
\arguments{
\item{X}{A list of information available at each stage in order, that is, \code{X[[1]]} represents the baseline information,
and \code{X[[t]]} is the information observed before \eqn{t^{th}} intervention.
The dimensionality of each element \code{X[[i]]} can be different from each other.
Notably, it can includes previous stages' action information \code{A} and outcome/reward information
\code{Y}. User can flexibly manipulate which covariates to use in training.
However, if argument \code{all.inclusive} is \code{TRUE}, all previous stages' \code{X}, \code{A},
and \code{Y} will be used in training. So, in that case, \code{X} should not involve action and reward
information.}

\item{A}{A list of actions taken during the sequential studies. The order should match with that of \code{X}}

\item{Y}{A list of outcomes observed in the sequential studies. The order should match with that of \code{X}.
\code{Y[[t]]} is suppose to be driven by the \code{X[[t]]} and action \code{A[[t]]}.}

\item{weights}{Weights on each stage of rewards. Default is all 1.}

\item{baseLearner}{Choose one baselearner for meta-learn er algorithms. So far, it supports \code{BART} by
package \code{dbarts}, \code{RF} (random forests) by \code{ranger}, \code{XGBoost} by
package \code{xgboost}, and \code{GAM} (generalized additive model)
through package \code{glmnet}, which can provide variable selection/sparsity by
various type of regularization. So more in details.}

\item{metaLearners}{\code{c("S")}. For continuous treatment assignment, only S-learner type is available.}

\item{include.X}{0 for no past X included in analysis; 1 for all past X included}

\item{include.A}{0 for no past treatment assignment included in analysis; 1 for only last A included; 2 for all past
A included}

\item{include.Y}{0 for no past reward/outcome Y included in analysis; 1 for only last Y included; 2 for all past
Y included}

\item{est.sigma}{Initial estimation of sigma. Only for T-learner with BART. If sample size is not enough to estimate
surface separately, or algorithm experience some trouble in getting sigma, use this argument to
provide an initial estimate.}

\item{parallel}{A boolean, for whether parallel computing is adopted. Also, if a numeric value, it implies the
number of cores to use. Otherwise, directly use the number from \code{detectCores()}}

\item{verbose}{Console print allowed?}

\item{...}{Additional arguments that can be passed to \code{dbarts::bart}, \code{ranger::ranger},
\code{params} of \code{xbgoost::xgb.cv}, or \code{glmnet::cv.glmnet}}
}
\value{
It includes learning results, basically, the trained functions that can be used for predictions. Since
sequential recommendations might require intermediate observations, \link{learnDTR.cont} will not automatically
provide prediction. But by another function \link{recommendDTR.cont},
predictions can flexibly been made stage by stage.
}
\description{
This function supports to learn the optimal sequential decision rules from either randomized studies
or observational ones. Multiple treatment options are supported.
}
\details{
This function supports to find the optimal dynamic treatment regime (DTR) for either randomized experiments
or observational studies. Also, thanks to meta-learner structure, S-, T-, and deC-learner can naturally
support multiple action options at any stage. \cr
\cr
For \code{GAM}, the algorithm will not automatically project the covariates \code{X} or outcomes/rewards
\code{Y} onto any bases-spanned spaces. User shall transform the covariates and/or  outcomes/rewards
manually and then input the desired design matrix through inputs \code{X} and/or \code{Y}.\cr
\cr
It is strongly suggested to adopt BART over random forests as baselearner if sample size is small.
}
\examples{
## this is the sample adopted in:
## https://jzhou.org/posts/optdtr/#case-1-random-assignment-with-two-treatment-options
DTRs = learnDTR.cont(X = ThreeStg_Dat$X,
                     A = ThreeStg_Dat$A,
                     Y = ThreeStg_Dat$Y,
                     weights = rep(1, 3),
                     baseLearner  = c("BART"),
                     metaLearners = c("S"),
                     include.X = 1,
                     include.A = 2,
                     include.Y = 0)
}
\references{
Künzel, Sören R., Jasjeet S. Sekhon, Peter J. Bickel, and Bin Yu.
"Metalearners for estimating heterogeneous treatment effects using machine learning."
\emph{Proceedings of the national academy of sciences} 116, no. 10 (2019): 4156-4165.
\cr\cr
Zhou, Junyi, Ying Zhang, and Wanzhu Tu. "A reference-free R-learner for treatment recommendation."
\emph{Statistical Methods in Medical Research} (2022)
}
\seealso{
\code{\link{recommendDTR.cont}}
}
\author{
Junyi Zhou \email{junyzhou@iu.edu}
}
