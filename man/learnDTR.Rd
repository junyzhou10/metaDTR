% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/learnDTR.R
\name{learnDTR}
\alias{learnDTR}
\title{Learning DTR from Sequential Interventions}
\usage{
learnDTR(
  X,
  A,
  Y,
  weights = rep(1, length(X)),
  baseLearner = c("BART", "GAM"),
  metaLearners = c("S", "T", "deC"),
  all.inclusive = FALSE
)
}
\arguments{
\item{X}{A list of information available at each stage in order, that is, \code{X[[1]]} represents the baseline information,
and \code{X[[t]]} is the information observed before \eqn{t^th} intervention.
The dimensionality of each element \code{X[[i]]} can be different from each other.
Notably, it can includes previous stages' action information \code{A} and outcome/reward information
\code{Y}. User can flexibly manipulate which covariates to use in training.
However, if argument \code{all.inclusive} is \code{TRUE}, all previous stages' \code{X}, \code{A},
and \code{Y} will be used in training. So, in that case, \code{X} shiould not involve action and reward
information.}

\item{A}{A list of actions taken during the sequantial studies. The order should match with that of \code{X}}

\item{Y}{A list of outcomes observed in the sequantial studies. The order should match with that of \code{X}.
\code{Y[[t]]} is suppose to be driven by the \code{X[[t]]} and action \code{A[[t]]}.}

\item{weights}{Weights on each stage of rewards. Default is all 1.}

\item{baseLearner}{Choose one baselearner for meta-learn er algorithms. So far supports \code{BART} and
\code{GAM} with splines and LASSO penalization.}

\item{metaLearners}{Meta-learner algorithms to learn the optimal DTR. To support more than two actions
at each stage, S-, A-, and deC-learner are available. But deC-learner only works when
\code{baseLearner = "GAM"} so far.}

\item{all.inclusive}{If \code{TRUE}, covariates adopted in training at stage \code{T} includes
\code{X[[t]], t<=T}, \code{A[[t]], t<T}, and \code{Y[[t]], t<T}. In other words,
\code{X} actually should only store additional covariates at each stage. Note that if
there are subjects dropping out during the study, it will cause error.}
}
\value{
It includes learning results, basically, the trained functions that can be used for predictions. Since
sequential recommendations might require intermediate observations, \code{\link[=learnDTR]{learnDTR()}} will not automatically
provide prediction. But by another function \link{recommendDTR},
predictions can flexibly been made stage by stage.
}
\description{
This function supports to learn the optimal sequential decision rules from either randomized studies
or observational ones. Multiple treatment options are supported.
}
\details{
This function supports to find the optimal dynamic treatment regime (DTR) for either randomized experiments
or observational studies. Also, thanks to meta-learner structure, S-, T-, and deC-learner can naturally
support multiple action options at any stage.
}
\seealso{
\code{\link{recommendDTR}}
}
\author{
Junyi Zhou \email{junyzhou@iu.edu}
}
